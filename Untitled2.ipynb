{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/X2HdjwLumzhHls2ecO7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabulhasan/CIS4400/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRBYcQoKzytt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# If using the native Google BigQuery API module:\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "import pandas as pd\n",
        "import os\n",
        "import pyarrow\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import credentials\n",
        "from google.oauth2 import service_account\n",
        "# If using a service account key file, save the path to that file in credentials.py and import credentials\n",
        "path_to_service_account_key_file = \"path_to_service_key.json\"\n",
        "#!pip install credentials\n",
        "# Set the name of the dimension\n",
        "dimension_name = 'agency'\n",
        "\n",
        "# Set the name of the surrogate key\n",
        "surrogate_key = f\"{dimension_name}_dim_id\"\n",
        "\n",
        "# Set the name of the business key\n",
        "business_key = f'{dimension_name}_id'\n",
        "\n",
        "# Set the GCP Project, dataset and table name\n",
        "gcp_project = 'gcp_project_name'\n",
        "bq_dataset = 'gcp_project_dataset'\n",
        "table_name = f\"{dimension_name}_dimension\"\n",
        "# Construct the full BigQuery path to the table\n",
        "dimension_table_path = f\"{gcp_project}.{bq_dataset}.{table_name}\"\n",
        "\n",
        "# Set the path to the source data files. Use double-slash for Windows paths C:\\\\myfolder\n",
        "# For Linux use forward slashes    /home/username/python_etl\n",
        "# For Mac use forward slashes      /users/username/python_etl\n",
        "# file_source_path = 'c:\\\\Python_ETL'\n",
        "# file_source_path = 'C:\\\\Users\\\\rholo\\\\OneDrive\\\\Documents\\\\classes\\\\4400\\\\311'\n",
        "file_source_path = '/Contnet'\n",
        "def transform_data(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    transform_data\n",
        "    Accepts a data frame\n",
        "    Performs any specific cleaning and transformation steps on the dataframe\n",
        "    Returns the modified dataframe\n",
        "    This function can be modified based on required changes\n",
        "    \"\"\"\n",
        "    # Select the columns for this dimension\n",
        "    column_list = ['agency','agency_name']\n",
        "    df = df[column_list]\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates()\n",
        "    return df\n",
        "def create_bigquery_client():\n",
        "    \"\"\"\n",
        "    create_bigquery_client\n",
        "    Creates a BigQuery client using the path to the service account key file\n",
        "    for credentials.\n",
        "    Returns the BigQuery client object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # If authenticating using a service account key file, use the following code:\n",
        "        # bqclient = bigquery.Client.from_service_account_json(credentials.path_to_service_account_key_file)\n",
        "        # Google Colab authentication already completed\n",
        "        bqclient = bigquery.Client(gcp_project)\n",
        "        return bqclient\n",
        "    except Exception as err:\n",
        "        print(\"error\")\n",
        "        # os._exit(-1)\n",
        "    return bqclient\n",
        "def upload_bigquery_table(bqclient, table_path, write_disposition, df):\n",
        "    \"\"\"\n",
        "    upload_bigquery_table\n",
        "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
        "    Loads the data into the BigQuery table from the dataframe.\n",
        "    for credentials.\n",
        "    The write disposition is either\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.\n",
        "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Set up a BigQuery job configuration with the write_disposition.\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
        "\n",
        "        # Submit the job\n",
        "        print(type(bqclient))\n",
        "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)\n",
        "        # Show the job results\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        #os._exit(-1)\n",
        "def bigquery_table_exists(bqclient, table_path):\n",
        "    \"\"\"\n",
        "    bigquery_table_exists\n",
        "    Accepts a path to a BigQuery table\n",
        "    Checks if the BigQuery table exists.\n",
        "    Returns True or False\n",
        "    \"\"\"\n",
        "    try:\n",
        "        bqclient.get_table(table_path)  # Make an API request.\n",
        "        return True\n",
        "    except NotFound:\n",
        "        return False\n",
        "def query_bigquery_table(table_path, bqclient, surrogate_key):\n",
        "    \"\"\"\n",
        "    query_bigquery_table\n",
        "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
        "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
        "    Returns the dataframe\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
        "    try:\n",
        "        bq_df = bqclient.query(sql_query).to_dataframe()\n",
        "    except Exception as err:\n",
        "        print(\"error\")\n",
        "    return bq_df\n",
        "def add_surrogate_key(df, dimension_name='customers', offset=1):\n",
        "    \"\"\"\n",
        "    add_surrogate_key\n",
        "    Accepts a data frame and inserts an integer identifier as the first column\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    # Reset the index to count from 0\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    # Add the new surrogate key starting from offset\n",
        "    df.insert(0, dimension_name+'_dim_id', df.index+offset)\n",
        "    return df\n",
        "def build_new_table(bqclient, dimension_table_path, dimension_name, df):\n",
        "    \"\"\"\n",
        "    build_new_table\n",
        "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
        "    Add the surrogate key and a record timestamp to the data frame\n",
        "    Inserts the contents of the dataframe to the dimensional table.\n",
        "    \"\"\"\n",
        "    # Add a surrogate key\n",
        "    df = add_surrogate_key(df, dimension_name, 1)\n",
        "    # Add the update timestamp\n",
        "    # Upload the dataframe to the BigQuery table\n",
        "    upload_bigquery_table(bqclient, dimension_table_path, \"WRITE_TRUNCATE\", df)\n",
        "# Program main\n",
        "# Load the CSV File into a dataframe\n",
        "# Transform the Dataframe\n",
        "# Create a BigQuery client\n",
        "# See if the target dimension table exists\n",
        "#    If not exists, load the data into a new table\n",
        "#    If exists, insert new records into the table\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.DataFrame\n",
        "    # Load in the data file\n",
        "    with open(file_source_path, 'r') as data:\n",
        "            df = pd.read_csv(data)\n",
        "        # Set all of the column names to lower case letters\n",
        "    df = df.rename(columns=str.lower)\n",
        "\n",
        "\n",
        "    #df = load_csv_data_file(file_source_path, \"my_311_data_WaterQuality.csv\", df)\n",
        "    # Transform the data\n",
        "    df = transform_data(df)\n",
        "    # Create the BigQuery Client\n",
        "    # setup enviroment parameters to connect to BQ project\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
        "\n",
        "    # Construct a BigQuery client object\n",
        "    bqclient = bigquery.Client()\n",
        "\n",
        "    # See if the target dimensional table exists\n",
        "    target_table_exists = bigquery_table_exists(bqclient, dimension_table_path  )\n",
        "\n",
        "    # If the target dimension table does not exist, load all of the data into a new table\n",
        "    if not target_table_exists:\n",
        "        build_new_table( bqclient, dimension_table_path, dimension_name, df)\n",
        "    # If the target table exists, then perform an incremental load\n",
        "\n",
        ""
      ]
    }
  ]
}